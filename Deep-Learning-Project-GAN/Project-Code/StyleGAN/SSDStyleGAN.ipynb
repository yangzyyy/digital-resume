{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec830d21-6f37-44df-9992-5d8893961eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from math import log2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pylab as plt\n",
    "from PIL import Image\n",
    "from numpy import random\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from dataset import dataset_split_shape\n",
    "from fft_feature import get_fft_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74214e0a-0453-4493-9c97-3e1750788cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET = 'CIFAR10'\n",
    "\n",
    "START_TRAIN_IMG_SIZE = 4\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "LR = 1e-3\n",
    "\n",
    "BATCH_SIZES =[64,64,32,16]\n",
    "\n",
    "CHANNELS_IMG = 3\n",
    "\n",
    "Z_DIm = 512\n",
    "\n",
    "W_DIM =512\n",
    "\n",
    "IN_CHANNELS = 512\n",
    "\n",
    "LAMBDA_GP = 10\n",
    "\n",
    "EPOCH = 500\n",
    "\n",
    "PROGRESSIVE_EPOCHS = [EPOCH] * len(BATCH_SIZES)\n",
    "\n",
    "factors = [1,1,1,1/2,1/4,1/8, 1/16]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_loader(image_size):\n",
    "    batch_size = BATCH_SIZES[int(log2(image_size/4))]\n",
    "    dataset = dataset_split_shape(DATASET)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return loader, dataset\n",
    "\n",
    "class WSLinear(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "\n",
    "        self, in_features, out_features\n",
    "\n",
    "    ):\n",
    "\n",
    "        super(WSLinear,self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "        self.scale  = (2/in_features) ** 0.5\n",
    "\n",
    "        self.bias   = self.linear.bias\n",
    "\n",
    "        self.linear.bias = None\n",
    "\n",
    "\n",
    "\n",
    "        nn.init.normal_(self.linear.weight)\n",
    "\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        return self.linear(x * self.scale) + self.bias\n",
    "\n",
    "class AdaIN(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, w_dim):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.instance_norm = nn.InstanceNorm2d(channels)\n",
    "\n",
    "        self.style_scale   = WSLinear(w_dim, channels)\n",
    "\n",
    "        self.style_bias    = WSLinear(w_dim, channels)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x,w):\n",
    "\n",
    "        x = self.instance_norm(x)\n",
    "\n",
    "        style_scale = self.style_scale(w).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "        style_bias  = self.style_bias(w).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "        return style_scale * x + style_bias\n",
    "\n",
    "class PixenNorm(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(PixenNorm, self).__init__()\n",
    "\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def forward(self,x ):\n",
    "\n",
    "        return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True)+  self.epsilon)\n",
    "\n",
    "class MappingNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim, w_dim):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.mapping = nn.Sequential(\n",
    "\n",
    "            PixenNorm(),\n",
    "\n",
    "            WSLinear(z_dim, w_dim),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            WSLinear(w_dim, w_dim),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            WSLinear(w_dim, w_dim),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            WSLinear(w_dim, w_dim),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            WSLinear(w_dim, w_dim),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            WSLinear(w_dim, w_dim),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            WSLinear(w_dim, w_dim),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            WSLinear(w_dim, w_dim),\n",
    "\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        return self.mapping(x)\n",
    "\n",
    "class injectNoise(nn.Module):\n",
    "\n",
    "    def __init__(self, channels):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.zeros(1,channels,1,1))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        noise = torch.randn((x.shape[0], 1, x.shape[2], x.shape[3]), device = x.device)\n",
    "\n",
    "        return x + self.weight + noise\n",
    "\n",
    "class WSConv2d(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "\n",
    "        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "\n",
    "    ):\n",
    "\n",
    "        super(WSConv2d, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "        self.scale = (2 / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "\n",
    "        self.bias = self.conv.bias\n",
    "\n",
    "        self.conv.bias = None\n",
    "\n",
    "\n",
    "\n",
    "        # initialize conv layer\n",
    "\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.leaky(self.conv1(x))\n",
    "\n",
    "        x = self.leaky(self.conv2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class GenBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, w_dim):\n",
    "\n",
    "        super(GenBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = WSConv2d(in_channel, out_channel)\n",
    "\n",
    "        self.conv2 = WSConv2d(out_channel, out_channel)\n",
    "\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        self.inject_noise1 = injectNoise(out_channel)\n",
    "\n",
    "        self.inject_noise2 = injectNoise(out_channel)\n",
    "\n",
    "        self.adain1 = AdaIN(out_channel, w_dim)\n",
    "\n",
    "        self.adain2 = AdaIN(out_channel, w_dim)\n",
    "\n",
    "    def forward(self, x,w):\n",
    "\n",
    "        x = self.adain1(self.leaky(self.inject_noise1(self.conv1(x))), w)\n",
    "\n",
    "        x = self.adain2(self.leaky(self.inject_noise2(self.conv2(x))), w)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def SNLinear(*args, **kwargs):\n",
    "    x = nn.Linear(*args, **kwargs)\n",
    "    return nn.utils.spectral_norm(x)\n",
    "\n",
    "\n",
    "\n",
    "class SSD_Generator(nn.Module):\n",
    "\n",
    "    r\"\"\"\n",
    "\n",
    "    Base class for a generic unconditional generator model.\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "        nz (int): Noise dimension for upsampling.\n",
    "\n",
    "        ngf (int): Variable controlling generator feature map sizes.\n",
    "\n",
    "        bottom_width (int): Starting width for upsampling generator output to an image.\n",
    "\n",
    "        loss_type (str): Name of loss to use for GAN loss.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim, w_dim, in_channels, img_channels=3, **kwargs):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.starting_cte = nn.Parameter(torch.ones(1, in_channels, 4,4))\n",
    "\n",
    "        self.map = MappingNetwork(z_dim, w_dim)\n",
    "\n",
    "        self.initial_adain1 = AdaIN(in_channels, w_dim)\n",
    "\n",
    "        self.initial_adain2 = AdaIN(in_channels, w_dim)\n",
    "\n",
    "        self.initial_noise1 = injectNoise(in_channels)\n",
    "\n",
    "        self.initial_noise2 = injectNoise(in_channels)\n",
    "\n",
    "        self.initial_conv   = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.leaky          = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "\n",
    "        self.initial_rgb    = WSConv2d(\n",
    "\n",
    "            in_channels, img_channels, kernel_size = 1, stride=1, padding=0\n",
    "\n",
    "        )\n",
    "\n",
    "        self.prog_blocks, self.rgb_layers = (\n",
    "\n",
    "            nn.ModuleList([]),\n",
    "\n",
    "            nn.ModuleList([self.initial_rgb])\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(len(factors)-1):\n",
    "\n",
    "            conv_in_c  = int(in_channels * factors[i])\n",
    "\n",
    "            conv_out_c = int(in_channels * factors[i+1])\n",
    "\n",
    "            self.prog_blocks.append(GenBlock(conv_in_c, conv_out_c, w_dim))\n",
    "\n",
    "            self.rgb_layers.append(WSConv2d(conv_out_c, img_channels, kernel_size = 1, stride=1, padding=0))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    # def generate_images(self, netG, num_images, device=None):\n",
    "\n",
    "    def generate_images(self, num_images, device=None):\n",
    "\n",
    "        r\"\"\"\n",
    "\n",
    "        Generates num_images randomly.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            num_images (int): Number of images to generate\n",
    "\n",
    "            device (torch.device): Device to send images to.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            Tensor: A batch of generated images.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if device is None:\n",
    "\n",
    "            device = self.device\n",
    "\n",
    "\n",
    "\n",
    "        noise = torch.randn((num_images, self.nz), device=device)\n",
    "\n",
    "        # fake_images = netG.forward(noise)\n",
    "\n",
    "        fake_images = self.forward(noise)\n",
    "\n",
    "\n",
    "\n",
    "        return fake_images\n",
    "\n",
    "\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "\n",
    "        return torch.tanh(alpha * generated + (1-alpha ) * upscaled)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, noise, alpha, steps):\n",
    "\n",
    "        w = self.map(noise)\n",
    "\n",
    "        x = self.initial_adain1(self.initial_noise1(self.starting_cte),w)\n",
    "\n",
    "        x = self.initial_conv(x)\n",
    "\n",
    "        out = self.initial_adain2(self.leaky(self.initial_noise2(x)), w)\n",
    "\n",
    "\n",
    "\n",
    "        if steps == 0:\n",
    "\n",
    "            return self.initial_rgb(x)\n",
    "\n",
    "        \n",
    "\n",
    "        for step in range(steps):\n",
    "\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode = 'bilinear')\n",
    "\n",
    "            out      = self.prog_blocks[step](upscaled,w)\n",
    "\n",
    "\n",
    "\n",
    "        final_upscaled = self.rgb_layers[steps-1](upscaled)\n",
    "\n",
    "        final_out      = self.rgb_layers[steps](out)\n",
    "\n",
    "\n",
    "\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SSD_Discriminator(nn.Module):\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, in_channels, img_channels=3):\n",
    "\n",
    "        super(SSD_Discriminator, self).__init__()\n",
    "\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "\n",
    "\n",
    "        # here we work back ways from factors because the discriminator\n",
    "\n",
    "        # should be mirrored from the generator. So the first prog_block and\n",
    "\n",
    "        # rgb layer we append will work for input size 1024x1024, then 512->256-> etc\n",
    "\n",
    "        for i in range(len(factors) - 1, 0, -1):\n",
    "\n",
    "            conv_in = int(in_channels * factors[i])\n",
    "\n",
    "            conv_out = int(in_channels * factors[i - 1])\n",
    "\n",
    "            self.prog_blocks.append(ConvBlock(conv_in, conv_out))\n",
    "\n",
    "            self.rgb_layers.append(\n",
    "\n",
    "                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        # perhaps confusing name \"initial_rgb\" this is just the RGB layer for 4x4 input size\n",
    "\n",
    "        # did this to \"mirror\" the generator initial_rgb\n",
    "\n",
    "        self.initial_rgb = WSConv2d(\n",
    "\n",
    "            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "\n",
    "        )\n",
    "\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "\n",
    "        self.avg_pool = nn.AvgPool2d(\n",
    "\n",
    "            kernel_size=2, stride=2\n",
    "\n",
    "        )  # down sampling using avg pool\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # this is the block for 4x4 input size\n",
    "\n",
    "        self.final_block = nn.Sequential(\n",
    "\n",
    "            # +1 to in_channels because we concatenate from MiniBatch std\n",
    "\n",
    "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n",
    "\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n",
    "\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            WSConv2d(\n",
    "\n",
    "                in_channels, 1, kernel_size=1, padding=0, stride=1\n",
    "\n",
    "            ),  # we use this instead of linear layer\n",
    "\n",
    "        )\n",
    "\n",
    "        self.snlienars = [nn.Linear(2, 1), nn.Linear(3, 1),nn.Linear(9, 1),nn.Linear(20, 1)]\n",
    "\n",
    "        self.snlienars = nn.Sequential(*self.snlienars)\n",
    "        \n",
    "#         self.spectral = nn.ModuleList([])\n",
    "        \n",
    "#         for i in range(4):\n",
    "#             self.spectral.append(nn.Sequential(self.snlienars[i], nn.LeakyReLU(0.2)))\n",
    "\n",
    "    \n",
    "\n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "\n",
    "        \"\"\"Used to fade in downscaled using avg pooling and output from CNN\"\"\"\n",
    "\n",
    "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
    "\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def minibatch_std(self, x):\n",
    "\n",
    "        batch_statistics = (\n",
    "\n",
    "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "\n",
    "        )\n",
    "\n",
    "        # we take the std for each example (across all channels, and pixels) then we repeat it\n",
    "\n",
    "        # for a single channel and concatenate it with the image. In this way the discriminator\n",
    "\n",
    "        # will get information about the variation in the batch/image\n",
    "\n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "    def compute_probs(self, output_real, output_fake):\n",
    "\n",
    "        r\"\"\"\n",
    "\n",
    "        Computes probabilities from real/fake images logits.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            output_real (Tensor): A batch of output logits of shape (N, 1) from real images.\n",
    "\n",
    "            output_fake (Tensor): A batch of output logits of shape (N, 1) from fake images.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            tuple: Average probabilities of real/fake image considered as real for the batch.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        D_x = torch.sigmoid(output_real).mean().item()\n",
    "\n",
    "        D_Gz = torch.sigmoid(output_fake).mean().item()\n",
    "\n",
    "\n",
    "\n",
    "        return D_x, D_Gz\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "\n",
    "        fft_feature = get_fft_feature(x)\n",
    "\n",
    "        out_spectral = self.snlienars[steps](fft_feature)\n",
    "\n",
    "        \n",
    "\n",
    "        # where we should start in the list of prog_blocks, maybe a bit confusing but\n",
    "\n",
    "        # the last is for the 4x4. So example let's say steps=1, then we should start\n",
    "\n",
    "        # at the second to last because input_size will be 8x8. If steps==0 we just\n",
    "\n",
    "        # use the final block\n",
    "\n",
    "        cur_step = len(self.prog_blocks) - steps\n",
    "\n",
    "\n",
    "\n",
    "        # convert from rgb as initial step, this will depend on\n",
    "\n",
    "        # the image size (each will have it's on rgb layer)\n",
    "\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "\n",
    "\n",
    "\n",
    "        if steps == 0:  # i.e, image is 4x4\n",
    "\n",
    "            out = self.minibatch_std(out)\n",
    "\n",
    "            return self.final_block(out).view(out.shape[0], -1), out_spectral\n",
    "\n",
    "\n",
    "\n",
    "        # because prog_blocks might change the channels, for down scale we use rgb_layer\n",
    "\n",
    "        # from previous/smaller size which in our case correlates to +1 in the indexing\n",
    "\n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n",
    "\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "\n",
    "\n",
    "\n",
    "        # the fade_in is done first between the downscaled and the input\n",
    "\n",
    "        # this is opposite from the generator\n",
    "\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "\n",
    "\n",
    "        for step in range(cur_step + 1, len(self.prog_blocks)):\n",
    "\n",
    "            out = self.prog_blocks[step](out)\n",
    "\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        return self.final_block(out).view(out.shape[0], -1), out_spectral\n",
    "\n",
    "def generate_examples(gen, steps, epoch, n=100):\n",
    "\n",
    "    loc = f'saved_examples/{DATASET}/SSD'\n",
    "\n",
    "    gen.eval()\n",
    "\n",
    "    alpha = 1.0\n",
    "\n",
    "    if os.path.exists(loc):\n",
    "\n",
    "        shutil.rmtree(loc)\n",
    "\n",
    "    os.makedirs(loc)\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            noise = torch.randn(1, Z_DIm).to(DEVICE)\n",
    "\n",
    "            img = gen(noise, alpha, steps)\n",
    "\n",
    "            save_image(img*0.5+0.5, f\"{loc}/img_{i}.png\")\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "\n",
    "\n",
    "def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n",
    "\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    interpolated_images = real * beta\n",
    "\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate critic scores\n",
    "\n",
    "    mixed_scores = critic(interpolated_images, alpha, train_step)[0]\n",
    "\n",
    " \n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "\n",
    "    gradient = torch.autograd.grad(\n",
    "\n",
    "        inputs=interpolated_images,\n",
    "\n",
    "        outputs=mixed_scores,\n",
    "\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "\n",
    "        create_graph=True,\n",
    "\n",
    "        retain_graph=True,\n",
    "\n",
    "    )[0]\n",
    "\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "\n",
    "def loss_critic_f(critic_real, critic_fake, gp=0):\n",
    "\n",
    "    return (\n",
    "\n",
    "            -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "\n",
    "            + LAMBDA_GP * gp\n",
    "\n",
    "            + (0.001) * torch.mean(critic_real ** 2)\n",
    "\n",
    "        )\n",
    "\n",
    "def train_fn(\n",
    "\n",
    "    critic,\n",
    "\n",
    "    gen,\n",
    "\n",
    "    loader,\n",
    "\n",
    "    dataset,\n",
    "\n",
    "    step,\n",
    "\n",
    "    alpha,\n",
    "\n",
    "    opt_critic,\n",
    "\n",
    "    opt_gen,\n",
    "\n",
    "    it\n",
    "\n",
    "):\n",
    "\n",
    "    loop = tqdm(loader, position=0, leave=True)\n",
    "\n",
    "\n",
    "\n",
    "    for batch_idx, (real, _) in enumerate(loop):\n",
    "\n",
    "        real = real.to(DEVICE)\n",
    "\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        noise = torch.randn(cur_batch_size, Z_DIm).to(DEVICE)\n",
    "\n",
    "        fake  = gen(noise, alpha, step)\n",
    "\n",
    "        \n",
    "\n",
    "        critic_spatial_real, critic_spectral_real = critic(real, alpha, step)\n",
    "\n",
    "        critic_spatial_fake, critic_spectral_fake = critic(fake.detach(), alpha, step)\n",
    "\n",
    "        \n",
    "\n",
    "        gp = gradient_penalty(critic, real, fake, alpha, step, DEVICE)\n",
    "\n",
    "        \n",
    "\n",
    "        errC = loss_critic_f(critic_spectral_real, critic_spectral_fake)\n",
    "\n",
    "        \n",
    "\n",
    "        out_real = 0.5 * critic_spectral_real.detach() + 0.5 * critic_spatial_real\n",
    "\n",
    "        out_fake = 0.5 * critic_spectral_fake.detach() + 0.5 * critic_spatial_fake\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        errD = loss_critic_f(out_real, out_fake, 0.5*gp)\n",
    "\n",
    "        \n",
    "\n",
    "        loss_critic = errC+errD\n",
    "\n",
    "\n",
    "\n",
    "        critic.zero_grad()\n",
    "\n",
    "        loss_critic.backward()\n",
    "\n",
    "        opt_critic.step()\n",
    "\n",
    "        \n",
    "\n",
    "        gen_fake_spatial, gen_fake_spectral = critic(fake, alpha, step)\n",
    "\n",
    "        out_gen = 0.5*gen_fake_spectral.detach()+0.5*gen_fake_spatial\n",
    "\n",
    "        loss_gen = -torch.mean(out_gen)\n",
    "\n",
    "\n",
    "\n",
    "        gen.zero_grad()\n",
    "\n",
    "        loss_gen.backward()\n",
    "\n",
    "        opt_gen.step()\n",
    "\n",
    "\n",
    "\n",
    "        alpha += cur_batch_size / (\n",
    "\n",
    "            PROGRESSIVE_EPOCHS[step] * 0.5 * len(dataset)\n",
    "\n",
    "        )\n",
    "\n",
    "        alpha = min(alpha,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        loop.set_postfix(\n",
    "\n",
    "            gp = gp.item(),\n",
    "\n",
    "            loss_critic = loss_critic.item()\n",
    "\n",
    "        )\n",
    "\n",
    "        it += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return alpha\n",
    "\n",
    "def train():\n",
    "\n",
    "    gen = SSD_Generator(\n",
    "\n",
    "        Z_DIm, W_DIM, IN_CHANNELS, CHANNELS_IMG\n",
    "\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    critic = SSD_Discriminator(IN_CHANNELS, CHANNELS_IMG).to(DEVICE)\n",
    "\n",
    "    opt_gen = optim.Adam([{'params': [param for name, param in gen.named_parameters() if 'map' not in name]},\n",
    "\n",
    "                        {'params': gen.map.parameters(), 'lr': 1e-5}], lr=LR, betas =(0.0, 0.99))\n",
    "\n",
    "    opt_critic = optim.Adam(\n",
    "\n",
    "        critic.parameters(), lr= LR, betas =(0.0, 0.99)\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    critic.train()\n",
    "\n",
    "    alpha = 1e-7\n",
    "\n",
    "    step = int(log2(START_TRAIN_IMG_SIZE / 4))\n",
    "\n",
    "    \n",
    "\n",
    "    for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
    "\n",
    "      #max_Epoch\n",
    "\n",
    "        alpha = 1e-7\n",
    "\n",
    "        print('Curent image size: '+str(4*2**step))\n",
    "\n",
    "        loader, dataset = get_loader(4*2**step)\n",
    "\n",
    "        it =0\n",
    "\n",
    "\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/ {num_epochs}')\n",
    "\n",
    "            alpha, gp, loss_c = train_fn(\n",
    "\n",
    "                critic, gen, loader, dataset, step, alpha, opt_critic, opt_gen, it\n",
    "\n",
    "            )\n",
    "\n",
    "            it += len(loader)\n",
    "    \n",
    "        generate_examples(gen, step, epoch)\n",
    "        step +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97de28ba-ad8e-4aa6-98b8-94884fbd5194",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch (Local)",
   "language": "python",
   "name": "local-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
